{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5982d-4f3c-4ab9-9c23-af60d8490afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wheel\n",
    "\n",
    "# Install the UIC repo\n",
    "! pip install ucimlrepo\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from gplearn.genetic import SymbolicClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd \n",
    "import math\n",
    "from statistics import mean, stdev\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efe1a3-350b-41c9-a95d-6c7ab67dd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the databases\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch datasets\n",
    "iris = fetch_ucirepo(id=53) \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "magic_gamma_telescope = fetch_ucirepo(id=159) \n",
    "banknote_authentication = fetch_ucirepo(id=267)\n",
    "yeast = fetch_ucirepo(id=110)\n",
    "letter_recognition = fetch_ucirepo(id=59) \n",
    "\n",
    "datasets = [iris, wine_quality,rice_cammeo_and_osmancik,breast_cancer_wisconsin_original,\n",
    "            magic_gamma_telescope, banknote_authentication, yeast, letter_recognition]\n",
    "\n",
    "datasets_strings = ['iris', 'wine_quality','rice_cammeo_and_osmancik','breast_cancer_wisconsin_original',\n",
    "            'magic_gamma_telescope', 'banknote_authentication', 'yeast', 'letter_recognition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17a5aa-ca94-4402-b1dc-98e4b99c29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the classifiers\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# CART\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# C4.5\n",
    "from ohmt.trees.univariate.c4 import C45\n",
    "from ohmt.trees.splits.evaluation import gini\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# OC1\n",
    "from ohmt.trees.multivariate.oc1 import OC1\n",
    "from ohmt.trees.splits.evaluation import gini\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# SVM_ODT\n",
    "! pip install Stree\n",
    "from stree import Stree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6071d-d7cf-4749-8f6c-7aad934ed03e",
   "metadata": {},
   "source": [
    "# Experiment 1: Proposed method against Standard Decision Tree Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb369e-d6fc-4ba2-b532-4dc8cad01b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "# CART\n",
    "\n",
    "# Create an empty dataframe \n",
    "col_names = ['database', 'mean_score', 'mean_std','mean_size', 'size_std', 'ave_time','std_time']  \n",
    "cart_df  = pd.DataFrame(columns = col_names) \n",
    "cart_df['database'] = datasets_strings\n",
    "\n",
    "# Iterate over datasets\n",
    "acc_scores = []\n",
    "std_scores = []\n",
    "depth_scores = []\n",
    "depth_std_scores = []\n",
    "ave_time_scores = []\n",
    "std_time_scores = []\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Preprocess dataset, and split into training and test sets\n",
    "    # data as pandas dataframes \n",
    "    X = ds.data.features \n",
    "    y = ds.data.targets \n",
    "\n",
    "    # Drop empty entries, if any\n",
    "    empty_row_idx = [index for index, row in X.iterrows() if row.isnull().any()]\n",
    "    if len(empty_row_idx) >= 1:\n",
    "        X = X.drop(index=empty_row_idx)\n",
    "        y = y.drop(index=empty_row_idx)\n",
    "        \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3)    \n",
    "    dtc = DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "    acc_each_run = []\n",
    "    std_each_run = []\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            start_time = time.time()\n",
    "            model = dtc.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "        \n",
    "            score = cross_val_score(model, X_test, y_test, cv=10)\n",
    "            exec_duration = end_time-start_time\n",
    "            tree_depth = model.tree_.max_depth\n",
    "\n",
    "            acc_each_run.append(score.mean().round(3))\n",
    "            std_each_run.append(score.std().round(3))\n",
    "            depth_each_run.append(tree_depth)\n",
    "            time_each_run.append(exec_duration)\n",
    "\n",
    "    ave_score = mean(acc_each_run)\n",
    "    ave_std = mean(std_each_run)\n",
    "    ave_depth = mean(depth_each_run)\n",
    "    ave_depth_std = stdev(depth_each_run)\n",
    "    ave_time = mean(time_each_run)\n",
    "    time_std = stdev(time_each_run)\n",
    "\n",
    "    acc_scores.append(ave_score)\n",
    "    std_scores.append(ave_std)\n",
    "    depth_scores.append(ave_depth)\n",
    "    depth_std_scores.append(ave_depth_std)\n",
    "    ave_time_scores.append(ave_time)\n",
    "    std_time_scores.append(time_std)\n",
    "\n",
    "cart_df[ 'mean_score'] = acc_scores\n",
    "cart_df[ 'mean_std'] = std_scores\n",
    "cart_df[ 'mean_size'] = depth_scores\n",
    "cart_df[ 'size_std'] = [round(x,3) for x in depth_std_scores]\n",
    "cart_df[ 'ave_time'] = [round(x,3) for x in ave_time_scores]\n",
    "cart_df[ 'std_time'] = [round(x,3) for x in std_time_scores]\n",
    "cart_df.set_index(\"database\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a308e-ea93-4d26-a5fd-50a833d6f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "# C4.5\n",
    "\n",
    "# Create an empty dataframe \n",
    "col_names =  ['database', 'mean_score', 'mean_std','mean_size', 'size_std', 'ave_time','std_time'] \n",
    "c45_df  = pd.DataFrame(columns = col_names) \n",
    "c45_df['database'] = datasets_strings\n",
    "\n",
    "# Iterate over datasets\n",
    "acc_scores = []\n",
    "std_scores = []\n",
    "depth_scores = []\n",
    "depth_std_scores = []\n",
    "ave_time_scores = []\n",
    "std_time_scores = []\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Preprocess dataset, and split into training and test sets\n",
    "    # data as pandas dataframes \n",
    "    X = ds.data.features \n",
    "    y = ds.data.targets \n",
    "\n",
    "    # Drop empty entries, if any\n",
    "    empty_row_idx = [index for index, row in X.iterrows() if row.isnull().any()]\n",
    "    if len(empty_row_idx) >= 1:\n",
    "        X = X.drop(index=empty_row_idx)\n",
    "        y = y.drop(index=empty_row_idx)\n",
    "        \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    X = X.values[:, :-1]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    y = ordinal_encoder.fit_transform(y)\n",
    "    y =  y[:, -1].astype(int)\n",
    "\n",
    "    # Generate the 10 runs to calculate the accuracy and standard deviation\n",
    "    acc_each_run = []\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "    i = 1\n",
    "    while i < 11:\n",
    "        # Test and train sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=.3,\n",
    "                                                               stratify=y)\n",
    "        c45_tree = C45()\n",
    "        start_time = time.time()\n",
    "        c45_tree = c45_tree.fit(train_features, train_labels, max_depth=10, min_eps=0.000000000000001, min_samples=10,\n",
    "                    node_fitness_function=gini)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        tree_size = max(c45_tree.depth.values())\n",
    "        exec_duration = end_time-start_time\n",
    "        \n",
    "    \n",
    "        # Evaluate tree \n",
    "        predicted_test_labels = c45_tree.predict(test_features) \n",
    "        score = accuracy_score(test_labels, predicted_test_labels)\n",
    "    \n",
    "        acc_each_run.append(score)\n",
    "        depth_each_run.append(tree_size)\n",
    "        time_each_run.append(exec_duration)\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "    ave_score = mean(acc_each_run)\n",
    "    ave_std = stdev(acc_each_run)\n",
    "    ave_depth = mean(depth_each_run)\n",
    "    ave_depth_std = stdev(depth_each_run)\n",
    "    ave_time = mean(time_each_run)\n",
    "    time_std = stdev(time_each_run)\n",
    "\n",
    "    acc_scores.append(round(ave_score,3))\n",
    "    std_scores.append(round(ave_std,3))\n",
    "    depth_scores.append(round(ave_depth,3))\n",
    "    depth_std_scores.append(round(ave_depth_std,3))\n",
    "    ave_time_scores.append(round(ave_time,3))\n",
    "    std_time_scores.append(round(time_std,3))\n",
    "\n",
    "c45_df[ 'mean_score'] = acc_scores\n",
    "c45_df[ 'mean_std'] = std_scores\n",
    "c45_df[ 'mean_size'] = depth_scores\n",
    "c45_df[ 'size_std'] = depth_std_scores\n",
    "c45_df[ 'ave_time'] = ave_time_scores\n",
    "c45_df[ 'std_time'] = std_time_scores\n",
    "c45_df.set_index(\"database\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045827a-5237-4f4c-bf52-b63fa241bea2",
   "metadata": {},
   "source": [
    "# Experiment 2: Proposed method against Oblique Decision Tree Algorithms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83683337-11ba-4ade-9759-a811355b35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "# OC1\n",
    "\n",
    "# Create an empty dataframe \n",
    "col_names =  ['database', 'mean_score', 'mean_std','mean_size', 'size_std', 'ave_time','std_time'] \n",
    "oc1_df  = pd.DataFrame(columns = col_names) \n",
    "oc1_df['database'] = datasets_strings\n",
    "\n",
    "# Iterate over datasets\n",
    "acc_scores = []\n",
    "std_scores = []\n",
    "depth_scores = []\n",
    "depth_std_scores = []\n",
    "ave_time_scores = []\n",
    "std_time_scores = []\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Preprocess dataset, and split into training and test sets\n",
    "    # data as pandas dataframes \n",
    "    X = ds.data.features \n",
    "    y = ds.data.targets \n",
    "\n",
    "    # Drop empty entries, if any\n",
    "    empty_row_idx = [index for index, row in X.iterrows() if row.isnull().any()]\n",
    "    if len(empty_row_idx) >= 1:\n",
    "        X = X.drop(index=empty_row_idx)\n",
    "        y = y.drop(index=empty_row_idx)\n",
    "    \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    X = X.values[:, :-1]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    y = ordinal_encoder.fit_transform(y)\n",
    "    y =  y[:, -1].astype(int)\n",
    "\n",
    "    # Generate the 10 runs to calculate the accuracy and standard deviation\n",
    "    acc_each_run = []\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "    i = 1\n",
    "    while i < 11:\n",
    "        # Test and train sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=.3,\n",
    "                                                               stratify=y)\n",
    "        oc1_tree = OC1()\n",
    "        start_time = time.time()\n",
    "        oc1_tree = oc1_tree.fit(train_features, train_labels, max_depth=10, min_eps=0.000000000000001, min_samples=10,\n",
    "                    node_fitness_function=gini)\n",
    "        end_time = time.time()\n",
    "        tree_size = max(oc1_tree.depth.values())\n",
    "        exec_duration = end_time-start_time\n",
    "    \n",
    "        # Evaluate tree \n",
    "        predicted_test_labels = oc1_tree.predict(test_features) \n",
    "        score = accuracy_score(test_labels, predicted_test_labels)\n",
    "    \n",
    "        acc_each_run.append(score) \n",
    "        depth_each_run.append(tree_size)\n",
    "        time_each_run.append(exec_duration)\n",
    "        i += 1\n",
    "\n",
    "    ave_score = mean(acc_each_run)\n",
    "    ave_std = stdev(acc_each_run)\n",
    "    ave_depth = mean(depth_each_run)\n",
    "    ave_depth_std = stdev(depth_each_run)\n",
    "    ave_time = mean(time_each_run)\n",
    "    time_std = stdev(time_each_run)\n",
    "\n",
    "    acc_scores.append(round(ave_score,3))\n",
    "    std_scores.append(round(ave_std,3))\n",
    "    depth_scores.append(round(ave_depth,3))\n",
    "    depth_std_scores.append(round(ave_depth_std,3))\n",
    "    ave_time_scores.append(round(ave_time,3))\n",
    "    std_time_scores.append(round(time_std,3))\n",
    "\n",
    "oc1_df[ 'mean_score'] = acc_scores\n",
    "oc1_df[ 'mean_std'] = std_scores\n",
    "oc1_df[ 'mean_size'] = depth_scores\n",
    "oc1_df[ 'size_std'] = depth_std_scores\n",
    "oc1_df[ 'ave_time'] = ave_time_scores\n",
    "oc1_df[ 'std_time'] = std_time_scores\n",
    "oc1_df.set_index(\"database\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d846896-545c-4e39-8ed2-a95c12a0d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "#  SVM_ODT\n",
    "\n",
    "# Create an empty dataframe \n",
    "col_names =   ['database', 'mean_score', 'mean_std','mean_size', 'size_std', 'ave_time','std_time'] \n",
    "svm_tree_df  = pd.DataFrame(columns = col_names) \n",
    "svm_tree_df['database'] = datasets_strings\n",
    "\n",
    "# Iterate over datasets\n",
    "acc_scores = []\n",
    "std_scores = []\n",
    "depth_scores = []\n",
    "depth_std_scores = []\n",
    "ave_time_scores = []\n",
    "std_time_scores = []\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Preprocess dataset, and split into training and test sets\n",
    "    # data as pandas dataframes \n",
    "    X = ds.data.features \n",
    "    y = ds.data.targets \n",
    "\n",
    "    # Drop empty entries, if any\n",
    "    empty_row_idx = [index for index, row in X.iterrows() if row.isnull().any()]\n",
    "    if len(empty_row_idx) >= 1:\n",
    "        X = X.drop(index=empty_row_idx)\n",
    "        y = y.drop(index=empty_row_idx)\n",
    "        \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3) \n",
    "    svm_tree = Stree(random_state=1, max_depth=10, multiclass_strategy=\"ovr\")\n",
    "\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            start_time = time.time()\n",
    "            model = svm_tree.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "        \n",
    "            score = cross_val_score(model, X_test, y_test, cv=10)\n",
    "            exec_duration = end_time-start_time\n",
    "            tree_depth =model.max_depth\n",
    "\n",
    "            acc_each_run.append(score.mean().round(3))\n",
    "            std_each_run.append(score.std().round(3))\n",
    "            depth_each_run.append(tree_depth)\n",
    "            time_each_run.append(exec_duration)\n",
    "\n",
    "    ave_score = mean(acc_each_run)\n",
    "    ave_std = mean(std_each_run)\n",
    "    ave_depth = mean(depth_each_run)\n",
    "    ave_depth_std = stdev(depth_each_run)\n",
    "    ave_time = mean(time_each_run)\n",
    "    time_std = stdev(time_each_run)\n",
    "\n",
    "    acc_scores.append(ave_score)\n",
    "    std_scores.append(ave_std)\n",
    "    depth_scores.append(ave_depth)\n",
    "    depth_std_scores.append(ave_depth_std)\n",
    "    ave_time_scores.append(ave_time)\n",
    "    std_time_scores.append(time_std)\n",
    "\n",
    "svm_tree_df[ 'mean_score'] = acc_scores\n",
    "svm_tree_df[ 'mean_std'] = std_scores\n",
    "svm_tree_df[ 'mean_size'] = depth_scores\n",
    "svm_tree_df[ 'size_std'] = [round(x,3) for x in depth_std_scores]\n",
    "svm_tree_df[ 'ave_time'] = [round(x,3) for x in ave_time_scores]\n",
    "svm_tree_df.set_index(\"database\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7cf46-7be8-4f54-a545-026956ce347e",
   "metadata": {},
   "source": [
    "# Experiment 3: The Effect of Hybrid Initialisation of the First Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b5cbf-4667-4525-817a-59770e7d72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe \n",
    "col_names = ['database', 'mean_score', 'mean_std','mean_size', 'size_std', 'ave_time','std_time']  \n",
    "warm_started_df  = pd.DataFrame(columns = col_names) \n",
    "warm_started_df['database'] = datasets_strings\n",
    "\n",
    "# Iterate over datasets\n",
    "acc_scores = []\n",
    "std_scores = []\n",
    "depth_scores = []\n",
    "depth_std_scores = []\n",
    "ave_time_scores = []\n",
    "std_time_scores = []\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Preprocess dataset, and split into training and test sets\n",
    "    # data as pandas dataframes \n",
    "    X = ds.data.features \n",
    "    y = ds.data.targets \n",
    "\n",
    "    # Drop empty entries, if any\n",
    "    empty_row_idx = [index for index, row in X.iterrows() if row.isnull().any()]\n",
    "    if len(empty_row_idx) >= 1:\n",
    "        X = X.drop(index=empty_row_idx)\n",
    "        y = y.drop(index=empty_row_idx)\n",
    "        \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3)    \n",
    "\n",
    "    acc_each_run = []\n",
    "    std_each_run = []\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Initialise the SymbolicClassifier\n",
    "            function_set = ['add', 'sub', 'mul']\n",
    "            base_cl = SymbolicClassifier(population_size = 100, generations = 50, stopping_criteria = 0.01, function_set=function_set,\n",
    "                      p_crossover = 0.7,parsimony_coefficient=0.0005,p_subtree_mutation = 0.1, p_hoist_mutation = 0.05, p_point_mutation = 0.1,\n",
    "                      max_samples = 0.9, verbose = 0, warm_start=True)\n",
    "\n",
    "            # Wrap the base classifier with OneVsRestClassifier\n",
    "            ovr_classifier = OneVsRestClassifier(base_cl)\n",
    "            \n",
    "            # Train the classifier\n",
    "            ovr_classifier.fit(X_train,y_train)\n",
    "\n",
    "            end_time = time.time()\n",
    "        \n",
    "            score = cross_val_score(ovr_classifier, X_test, y_test, cv=10)\n",
    "            exec_duration = end_time-start_time\n",
    "            tree_depth = base_cl.genetic_.max_depth\n",
    "\n",
    "            acc_each_run.append(score.mean().round(3))\n",
    "            std_each_run.append(score.std().round(3))\n",
    "            depth_each_run.append(tree_depth)\n",
    "            time_each_run.append(exec_duration)\n",
    "\n",
    "    ave_score = mean(acc_each_run)\n",
    "    ave_std = mean(std_each_run)\n",
    "    ave_depth = mean(depth_each_run)\n",
    "    ave_depth_std = stdev(depth_each_run)\n",
    "    ave_time = mean(time_each_run)\n",
    "    time_std = stdev(time_each_run)\n",
    "\n",
    "    acc_scores.append(ave_score)\n",
    "    std_scores.append(ave_std)\n",
    "    depth_scores.append(ave_depth)\n",
    "    depth_std_scores.append(ave_depth_std)\n",
    "    ave_time_scores.append(ave_time)\n",
    "    std_time_scores.append(time_std)\n",
    "\n",
    "warm_started_df[ 'mean_score'] = acc_scores\n",
    "warm_started_df[ 'mean_std'] = std_scores\n",
    "warm_started_df[ 'mean_size'] = depth_scores\n",
    "warm_started_df[ 'size_std'] = [round(x,3) for x in depth_std_scores]\n",
    "warm_started_df[ 'ave_time'] = [round(x,3) for x in ave_time_scores]\n",
    "warm_started_df[ 'std_time'] = [round(x,3) for x in std_time_scores]\n",
    "warm_started_df.set_index(\"database\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6282c-afd5-4086-8e21-74641f80d0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
